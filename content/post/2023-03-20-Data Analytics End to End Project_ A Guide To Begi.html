
				<!DOCTYPE html>
				<html>
					<head>
						<meta charset="UTF-8">
						<meta name="viewport" content="width=device-width, initial-scale=1" />
						<link rel="stylesheet" href="pluginAssets/katex/katex.css"><script type="application/javascript" src="pluginAssets/mermaid/mermaid.min.js"></script><script type="application/javascript" src="pluginAssets/mermaid/mermaid_render.js"></script><link rel="stylesheet" href="pluginAssets/de.habelt.CodeSection/markdownItCodeSection.css"><link rel="stylesheet" href="pluginAssets/highlight.js/atom-one-light.css">
						<title>Data Analytics End to End Project: A Guide To Beginners in Data Analytics | by Prateek Gaurav | Medium</title>
					</head>
					<body>
						<div class="exported-note"><div class="exported-note-title">Data Analytics End to End Project: A Guide To Beginners in Data Analytics | by Prateek Gaurav | Medium</div>

<style>
		/* https://necolas.github.io/normalize.css/ */
		html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}
		article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}
		pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent;-webkit-text-decoration-skip:objects}
		b,strong{font-weight:bolder}small{font-size:80%}img{border-style:none}

		body {
			font-size: 15px;
			color: #32373F;
			word-wrap: break-word;
			line-height: 1.6em;
			background-color: #ffffff;
			font-family: 'Avenir', 'Arial', sans-serif;
			padding-bottom: 0px;
			padding-top: 0px;
		}
		kbd {
			border: 1px solid rgb(220, 220, 220);
			box-shadow: inset 0 -1px 0 rgb(220, 220, 220);
			padding: 2px 4px;
			border-radius: 3px;
			background-color: rgb(243, 243, 243);
		}
		::-webkit-scrollbar {
			width: 7px;
			height: 7px;
		}
		::-webkit-scrollbar-corner {
			background: none;
		}
		::-webkit-scrollbar-track {
			border: none;
		}
		::-webkit-scrollbar-thumb {
			background: rgba(100, 100, 100, 0.3); 
			border-radius: 5px;
		}
		::-webkit-scrollbar-track:hover {
			background: rgba(0, 0, 0, 0.1); 
		}
		::-webkit-scrollbar-thumb:hover {
			background: rgba(100, 100, 100, 0.7); 
		}

		

		/* Remove top padding and margin from first child so that top of rendered text is aligned to top of text editor text */

		#rendered-md > h1:first-child,
		#rendered-md > h2:first-child,
		#rendered-md > h3:first-child,
		#rendered-md > h4:first-child,
		#rendered-md > ul:first-child,
		#rendered-md > ol:first-child,
		#rendered-md > table:first-child,
		#rendered-md > blockquote:first-child,
		#rendered-md > img:first-child,
		#rendered-md > p:first-child {
			margin-top: 0;
			padding-top: 0;
		}
		
		p, h1, h2, h3, h4, h5, h6, ul, table {
			margin-top: .6em;
			margin-bottom: 1.35em;

			/*
				Adds support for RTL text in the note body. It automatically detects the direction using the content.
				Issue: https://github.com/laurent22/joplin/issues/3991
			*/
			unicode-bidi: plaintext;
		}

		h1, h2, h3, h4, h5, h6, ul, table {
			margin-bottom: 0.65em;
		}

		h1, h2, h3, h4, h5, h6 {
			line-height: 1.5em;
		}
		h1 {
			font-size: 1.5em;
			font-weight: bold;
			border-bottom: 1px solid #dddddd;
			padding-bottom: .3em;
		}
		h2 {
			font-size: 1.3em;
			font-weight: bold;
			padding-bottom: .1em; */
		}
		h3 {
			font-size: 1.1em;
			font-weight: bold;
		}
		h4, h5, h6 {
			font-size: 1em;
			font-weight: bold;
		}

		.exported-note-title {
			font-size: 2em;
			font-weight: bold;
			margin-bottom: 0.8em;
			line-height: 1.5em;
			padding-bottom: .35em;
			border-bottom: 1px solid #dddddd;
		}

		a {
			color: #155BDA;
		}
		ul, ol {
			padding-left: 0;
			margin-left: 1.7em;
		}
		li {
			margin-bottom: .4em;
		}
		li p {
			margin-top: 0.2em;
			margin-bottom: 0;
		}

		.resource-icon {
			display: inline-block;
			position: relative;
			top: 0.3em;
			text-decoration: none;
			width: 1.2em;
			height: 1.4em;
			margin-right: 0.4em;
			background-color:  #155BDA;
		}
    /* These icons are obtained from the wonderful ForkAwesome project by copying the src svgs 
     * into the css classes below.
     * svgs are obtained from https://github.com/ForkAwesome/Fork-Awesome/tree/master/src/icons/svg
     * instead of the svg width, height property you must use a viewbox here, 0 0 1536 1792 is typically the actual size of the icon
     * each line begins with the pre-amble -webkit-mask: url("data:image/svg+xml;utf8,
     * and of course finishes with ");
     * to precvent artifacts it is also necessary to include -webkit-mask-repeat: no-repeat;
     * on the following line
     * */
		.fa-joplin {
			/* Awesome Font file */
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M373.834 128C168.227 128 0 296.223 0 501.834v788.336C0 1495.778 168.227 1664 373.834 1664h788.336c205.608 0 373.83-168.222 373.83-373.83V501.834C1536 296.224 1367.778 128 1162.17 128zm397.222 205.431h417.424a7.132 7.132 0 0 1 7.132 7.133v132.552c0 4.461-3.619 8.073-8.077 8.073h-57.23c-24.168 0-43.768 19.338-44.284 43.374v2.377h-.017v136.191h-.053l-.466 509.375c-5.02 77.667-39.222 149.056-96.324 201.046-60.28 54.834-141.948 85.017-229.962 85.017-12.45 0-25.208-.61-37.907-1.785-92.157-8.682-181.494-48.601-251.662-112.438-71.99-65.517-117.147-150.03-127.164-238-11.226-98.763 23.42-192.783 95.045-257.937 81.99-74.637 198.185-101.768 316.613-75.704 5.574 1.227 9.55 6.282 9.55 11.997v199.52c-.199 2.625-1.481 6.599-8.183 2.896-.663-.365-1.194-.511-1.653-.531-21.987-10.587-45.159-17.57-68.559-19.916-.38-.04-.757-.124-1.138-.163-.537-.048-1.034-.033-1.556-.075-4.13-.354-8.183-.517-12.203-.58-.87-.011-1.771-.127-2.641-.127-.486 0-.951.05-1.437.057-1.464.011-2.886.115-4.33.163-2.76.102-5.497.211-8.182.448-.273.024-.547.07-.835.097-25.509 2.4-47.864 11.104-65.012 25.47-.954.802-1.974 1.53-2.9 2.36a1.34 1.34 0 0 1-.168.146c-23.96 21.8-34.881 53.872-30.726 90.316 4.62 40.737 26.94 81.156 62.841 113.823 35.908 32.67 80.335 52.977 125.113 57.186 35.118 3.36 66.547-3.919 89.899-20.461a97.255 97.255 0 0 0 9.365-7.501c2.925-2.661 5.569-5.5 8.086-8.416.3-.348.672-.673.975-1.024 8.253-9.864 14.222-21.067 17.996-33.148.639-2.034 1.051-4.148 1.564-6.227.381-1.563.81-3.106 1.112-4.693.555-2.784.923-5.632 1.253-8.49.086-.709.183-1.414.237-2.128.492-4.893.693-9.858.55-14.91h.013V521.623c-2.01-22.626-20.78-40.434-43.928-40.434h-57.23a8.071 8.071 0 0 1-8.077-8.073V340.564a7.132 7.132 0 0 1 7.136-7.133z'/></svg>");
		}
		.fa-file-image {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zm-128-448v320H256v-192l192-192 128 128 384-384zm-832-192c-106 0-192-86-192-192s86-192 192-192 192 86 192 192-86 192-192 192z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-pdf {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zm-514-593c25 20 53 38 84 56 42-5 81-7 117-7 67 0 152 8 177 49 7 10 13 28 2 52-1 1-2 3-3 4v1c-3 18-18 38-71 38-64 0-161-29-245-73-139 15-285 46-392 83-103 176-182 262-242 262-10 0-19-2-28-7l-24-12c-3-1-4-3-6-5-5-5-9-16-6-36 10-46 64-123 188-188 8-5 18-2 23 6 1 1 2 3 2 4 31-51 67-116 107-197 45-90 80-178 104-262-32-109-42-221-24-287 7-25 22-40 42-40h22c15 0 27 5 35 15 12 14 15 36 9 68-1 3-2 6-4 8 1 3 1 5 1 8v30c-1 63-2 123-14 192 35 105 87 190 146 238zm-576 411c30-14 73-57 137-158-75 58-122 124-137 158zm398-920c-10 28-10 76-2 132 3-16 5-31 7-44 2-17 5-31 7-43 1-3 2-5 4-8-1-1-1-3-2-5-1-18-7-29-13-36 0 2-1 3-1 4zm-124 661c88-35 186-63 284-81-10-8-20-15-29-23-49-43-93-103-127-176-19 61-47 126-83 197-15 28-30 56-45 83zm646-16c-5-5-31-24-140-24 49 18 94 28 124 28 9 0 14 0 18-1 0-1-1-2-2-3z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-word {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zM233 768v107h70l164 661h159l128-485c5-15 8-30 10-46 1-8 2-16 2-24h4l3 24c3 14 4 30 9 46l128 485h159l164-661h70V768h-300v107h90l-99 438c-4 16-6 33-7 46l-2 21h-4c0-6-2-14-3-21-3-13-5-30-9-46L825 768H711l-144 545c-4 16-5 33-8 46l-4 21h-4l-2-21c-1-13-3-30-7-46l-99-438h90V768H233z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-powerpoint {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zm-992-234v106h327v-106h-93v-167h137c43 0 82-2 118-15 90-31 146-124 146-233s-54-193-137-228c-38-15-84-19-130-19H416v107h92v555h-92zm353-280H650V882h120c35 0 62 6 83 18 36 21 56 62 56 115 0 56-20 99-62 120-21 10-47 15-78 15z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-excel {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zm-979-234v106h281v-106h-75l103-161c12-19 18-34 21-34h2c1 4 3 7 5 10 4 8 10 14 17 24l107 161h-76v106h291v-106h-68l-192-273 195-282h67V768H828v107h74l-103 159c-12 19-21 34-21 33h-2c-1-4-3-7-5-10-4-7-9-14-17-23L648 875h76V768H434v107h68l189 272-194 283h-68z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-audio {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zM620 850c12 5 20 17 20 30v544c0 13-8 25-20 30-4 1-8 2-12 2-8 0-16-3-23-9l-166-167H288c-18 0-32-14-32-32v-192c0-18 14-32 32-32h131l166-167c10-9 23-12 35-7zm417 689c19 0 37-8 50-24 83-102 129-231 129-363s-46-261-129-363c-22-28-63-32-90-10-28 23-32 63-9 91 65 80 100 178 100 282s-35 202-100 282c-23 28-19 68 9 90 12 10 26 15 40 15zm-211-148c17 0 34-7 47-20 56-60 87-137 87-219s-31-159-87-219c-24-26-65-27-91-3-25 24-27 65-2 91 33 36 52 82 52 131s-19 95-52 131c-25 26-23 67 2 91 13 11 29 17 44 17z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-video {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zM768 768c70 0 128 58 128 128v384c0 70-58 128-128 128H384c-70 0-128-58-128-128V896c0-70 58-128 128-128h384zm492 2c12 5 20 17 20 30v576c0 13-8 25-20 30-4 1-8 2-12 2-8 0-17-3-23-9l-265-266v-90l265-266c6-6 15-9 23-9 4 0 8 1 12 2z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-archive {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M640 384V256H512v128h128zm128 128V384H640v128h128zM640 640V512H512v128h128zm128 128V640H640v128h128zm700-388c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H768v128H640V128H128v1536h1280zM781 943c85 287 107 349 107 349 5 17 8 34 8 52 0 111-108 192-256 192s-256-81-256-192c0-18 3-35 8-52 0 0 21-62 120-396V768h128v128h79c29 0 54 19 62 47zm-141 465c71 0 128-29 128-64s-57-64-128-64-128 29-128 64 57 64 128 64z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-code {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zM480 768c11-14 31-17 45-6l51 38c14 11 17 31 6 45l-182 243 182 243c11 14 8 34-6 45l-51 38c-14 11-34 8-45-6l-226-301c-8-11-8-27 0-38zm802 301c8 11 8 27 0 38l-226 301c-11 14-31 17-45 6l-51-38c-14-11-17-31-6-45l182-243-182-243c-11-14-8-34 6-45l51-38c14-11 34-8 45 6zm-620 461c-18-3-29-20-26-37l138-831c3-18 20-29 37-26l63 10c18 3 29 20 26 37l-138 831c-3 18-20 29-37 26z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file-alt, .fa-file-csv {
      /* fork-awesome doesn't have csv so we use the text icon */
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280zM384 800c0-18 14-32 32-32h704c18 0 32 14 32 32v64c0 18-14 32-32 32H416c-18 0-32-14-32-32v-64zm736 224c18 0 32 14 32 32v64c0 18-14 32-32 32H416c-18 0-32-14-32-32v-64c0-18 14-32 32-32h704zm0 256c18 0 32 14 32 32v64c0 18-14 32-32 32H416c-18 0-32-14-32-32v-64c0-18 14-32 32-32h704z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		.fa-file {
			-webkit-mask: url("data:image/svg+xml;utf8,<svg viewBox='0 0 1536 1792' xmlns='http://www.w3.org/2000/svg'><path d='M1468 380c37 37 68 111 68 164v1152c0 53-43 96-96 96H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h896c53 0 127 31 164 68zm-444-244v376h376c-6-17-15-34-22-41l-313-313c-7-7-24-16-41-22zm384 1528V640H992c-53 0-96-43-96-96V128H128v1536h1280z'/></svg>");
      -webkit-mask-repeat: no-repeat;
		}
		blockquote {
			border-left: 4px solid rgb(220, 220, 220);
			padding-left: 1.2em;
			margin-left: 0;
			opacity: 0.7;
		}

		.jop-tinymce table,
		table {
			text-align: left;
			border-collapse: collapse;
			border: 1px solid rgb(220, 220, 220);
			background-color: #ffffff;
		}

		.jop-tinymce table td, .jop-tinymce table th,
		table td, th {
			text-align: left;
			padding: .5em 1em .5em 1em;
			font-size: 15;
			color: #32373F;
			font-family: 'Avenir', 'Arial', sans-serif;
		}

		.jop-tinymce table td,
		table td {
			border: 1px solid rgb(220, 220, 220);
		}

		.jop-tinymce table th,
		table th {
			border: 1px solid rgb(220, 220, 220);
			border-bottom: 2px solid rgb(220, 220, 220);
			background-color: rgb(247, 247, 247);
		}

		.jop-tinymce table tr:nth-child(even),
		table tr:nth-child(even) {
			background-color: rgb(247, 247, 247);
		}

		.jop-tinymce table tr:hover,
		table tr:hover {
			background-color: #e5e5e5;
		}

		hr {
			border: none;
			border-bottom: 2px solid #dddddd;
		}
		img {
			max-width: 100%;
			height: auto;
		}
		
		.inline-code,
		.mce-content-body code {
			border: 1px solid rgb(220, 220, 220);
			background-color: rgb(243, 243, 243);
			padding-right: .2em;
			padding-left: .2em;
			border-radius: .25em;
			color: rgb(0,0,0);
			font-size: .9em;
		}

		.highlighted-keyword {
			background-color: #F3B717;
			color: black;
		}

		.not-loaded-resource img {
			width: 1.15em;
			height: 1.15em;
			background: white;
			padding: 2px !important;
			border-radius: 2px;
			box-shadow: 0 1px 3px #000000aa;
		}

		a.not-loaded-resource img {
			margin-right: .2em;
		}

		a.not-loaded-resource {
			display: flex;
			flex-direction: row;
			align-items: center;
		}

		.md-checkbox input[type=checkbox]:checked {
			opacity: 0.7;
		}

		.jop-tinymce ul.joplin-checklist .checked,
		.md-checkbox .checkbox-label-checked {
			opacity: 0.5;
		}

		.exported-note {
			padding: 1em;
		}

		.joplin-editable .joplin-source {
			display: none;
		}

		mark {
			background: #F7D26E;
			color: black;
		}

		/* =============================================== */
		/* For TinyMCE */
		/* =============================================== */

		.mce-content-body {
			/* Note: we give a bit more padding at the bottom, to allow scrolling past the end of the document */
			padding: 5px 10px 10em 0;
		}

		/*
		.mce-content-body code {
			background-color: transparent;
		}
		*/

		.mce-content-body [data-mce-selected=inline-boundary] {
			background-color: transparent;
		}

		.mce-content-body .joplin-editable {
			cursor: pointer !important;
		}

		.mce-content-body.mce-content-readonly {
			opacity: 0.5;
		}

		/* We need that to make sure click events have the A has a target */
		.katex a span {
			pointer-events: none;
		}

		.media-player {
			width: 100%;
			margin-top: 10px;
		}

		.media-player.media-pdf {
			min-height: 35rem;
			width: 100%;
			max-width: 1000px;
			margin: 0;
			border: 0;
			display: block;
		}

		/* Clear the CODE style if the element is within a joplin-editable block */
		.mce-content-body .joplin-editable code {
			border: none;
			background: none;
			padding: 0;
			color: inherit;
			font-size: inherit;
		}

		/* To make code blocks horizontally scrollable */
		/* https://github.com/laurent22/joplin/issues/5740 */
		pre.hljs {
			overflow-x: auto;
		}

		/* =============================================== */
		/* For TinyMCE */
		/* =============================================== */

		@media print {
			body {
				height: auto !important;
			}

			pre {
				white-space: pre-wrap;
			}

			.code, .inline-code {
				border: 1px solid #CBCBCB;
			}

			#joplin-container-content {
				/* The height of the content is set dynamically by JavaScript (in updateBodyHeight) to go
				   around various issues related to scrolling. However when printing we don't want this
				   fixed size as that would crop the content. So we set it to auto here. "important" is
				   needed to override the style set by JavaScript at the element-level. */
				height: auto !important;
			}
		}
	

				/*
					FOR THE MARKDOWN EDITOR
				*/

				/* Remove the indentation from the checkboxes at the root of the document
				   (otherwise they are too far right), but keep it for their children to allow
				   nested lists. Make sure this value matches the UL margin. */

				li.md-checkbox {
					list-style-type: none;
				}

				li.md-checkbox input[type=checkbox] {
					margin-left: -1.71em;
					margin-right: 0.7em;
				}
				
				ul.joplin-checklist {
					list-style:none;
				}

				/*
					FOR THE RICH TEXT EDITOR
				*/

				ul.joplin-checklist li::before {
					content:"\f14a";
					font-family:"Font Awesome 5 Free";
					background-size: 16px 16px;
					pointer-events: all;
					cursor: pointer;
					width: 1em;
					height: 1em;
					margin-left: -1.3em;
					position: absolute;
					color: #32373F;
				}

				.joplin-checklist li:not(.checked)::before {
					content:"\f0c8";
				}
.mermaid { background-color: white; width: 640px; }</style><div id="rendered-md"><h1 id="data-analytics-end-to-end-project-a-guide-to-beginners-in-data-analytics">Data Analytics End to End Project: A Guide To Beginners in Data Analytics</h1>
<p>Data Science is indeed a hot topic for the last few years since it was mentioned as the sexiest job of the 21st Century. Indeed Data Science is awesome and has a lot of scopes, I will compare this scenario to what the computer industry was in the 1900s and early 2000s, it is similar to the Data Science industry right now.<br>
A lot of people from different fields are switching into Analytics and Data Science field, a lot of fresh graduates are trying to get a job in this field but the companies expect the hires should have prior experience in this field or they should have worked on some similar projects.<br>
I wanted to write this article to help such people using my past experience. This article contains the use of almost every important tool and knowledge used in the Data Analytics project in the industry except for the use of Microsoft Excel, which I believe is a very crucial skill if you want to work in the Analytics field.</p>
<p>The steps of Data Science are as follows in the mentioned order:<br>
1. Problem Statement<br>
2. Data Collection<br>
3. Data Cleaning<br>
4. Data Analysis and Visualization<br>
5. Modelling (Machine Learning Part)<br>
I wanted to name this article as Data Science End to End Project. But I thought that will not be a good idea as I will not be discussing the modeling part in this article. Hence I named this as Data Analytics End to End Project as it covers almost every part of a Data Analysis process from Data Collection to Data Analysis.</p>
<p>Here are the steps we will follow in this article:<br>
1. We will come up with a Problem Statement.<br>
2. We will collect the data using an API which will run on a cronjob every hour on an EC2 virtual machine and store the data collected on AWS RDS.<br>
3. Then we will see how to clean this data.<br>
4. Finally using Tableau, we will try to find some insights from this data.</p>
<p>This will be an end-to-end project which will start from a problem statement to getting the insight. You can follow these steps to replicate the same project or maybe come up with your own problem statement, find the API to get data for that problem to collect the data (the source of data collection might be different. You can use web scraping or survey or any other source), create a database either on cloud or local system and finally use this data to find the insights using Python or Tableau.</p>
<h2 id="learn-more-about-data-science-from-udacity"><a data-from-md title='https://imp.i115008.net/QOvQ13' href='https://imp.i115008.net/QOvQ13'>Learn more about Data Science from Udacity.</a></h2>
<p>** This is an affiliate link, meaning if you enroll in any course on Udacity using my URL, I will get a referral bonus.</p>
<h1 id="step-1-problem-statement">Step 1: Problem Statement</h1>
<p>I started this project in the month of September 2019. I wanted to do a project where I will cover every part of the Data Analysis process. When you work in organizations, you don’t really get to work on end-to-end projects.</p>
<p>The problem statement might be coming from an MBA guy or senior management in the company. The data collection part will be taken care of by a different team. There might be a different team for Data Cleaning Process. Finally, a different team for using this cleaned data to do data visualization or analysis.</p>
<p>But if you are in a startup, the scenario might be different. You will be responsible for multiple roles in this process. But if you know the A to Z of this process and have the hands-on experience, it will be beneficial. A startup would like to hire you as you will be able to manage multiple roles. A big organization would be happy to hire you, as they will be able to use you in multiple roles as per the client’s requirement.</p>
<p>Coming back to the project which I started in September 2019. The hot topic in India around the Month of October/November is Pollution. In late September and October each year, farmers mainly in Punjab and Haryana burn an estimated 35 million tons of crop waste from their paddy fields after harvesting as a low-cost straw-disposal practice to reduce the turnaround time between harvesting and sowing for the second (winter) crop. Also, during this period festivals like Dussehra and Diwali are celebrated in India. During these festivals, a lot of crackers are burnt which also affects the pollution level. In cities like Delhi, the PM 2.5 level goes above 999 which is a very serious issue. I wanted to understand the effect of different times and events on the pollution level in different parts of India.</p>
<p>So, in short, my problem statement was:<br>
<strong>To understand the effect of different times and events on the pollution level in different parts of India.</strong></p>
<h1 id="step-2-data-collection">Step 2: Data Collection</h1>
<p>There are a lot of ways to collect the data like:</p>
<ol>
<li>Through API — Mostly this is a cleaned form of data and does not require further cleaning.</li>
<li>Web Scraping — Most of the time you will need to spend a good amount of time cleaning such data.</li>
<li>Transactional Data</li>
<li>Online Marketing Analytics Data (Customer Journey, etc.)</li>
<li>Surveys and Interviews, etc.. etc...</li>
</ol>
<p>The best way to get the data for Data Analysis or Visualization according to me would be to <strong>use an API</strong> as mostly they are clean. But if you are an offline or online store like Walmart or Amazon, you will be having huge transactional data. A good DBA would make sure to keep this data in the cleaned format, so an Analyst will only have to query the data according to the requirement from this huge pile of data and do the analysis. Even the Online Marketing Analytics or Survey Data will be in a cleaned format, but methods like Web Scraping can consume a lot of time in Data Cleaning. You can write all the cleaning codes in the script only so that the final scraped output you get is clean and ready to use. The biggest pain is cleaning the text data, I have worked on it can be really frustrating sometimes.</p>
<p>So for this article, I have used an API provided by the Government Of India on their website <a data-from-md title='https://data.gov.in/' href='https://data.gov.in/'>https://data.gov.in/</a>, I found this API where they were providing Air Pollution data for about 1200 places across different cities and states in India on an hourly basis. Different metrics like PM2.5, PM10, NO2, NH3, SO2, CO, and Ozone’s Maximum, Minimum, and Average values were collected.</p>
<p>This data was provided on an hourly basis, so if you hit this API at 3 PM, it will provide you the data collected till 2 PM for the 1200 places, but if you hit this API at 5 PM you will get the data for 4 PM. Hence you cannot get the past data from this API, so you have to create a DB to append the new data every hour.</p>
<h1 id="2a-collecting-the-data-in-database-or-google-sheets">2a. Collecting the Data in Database or Google Sheets</h1>
<p>There are two ways you can collect this data. Either you can use Google Sheets as your Database and keep on appending the new data or you can create a Database using AWS RDS. Initially, I started with using Google Sheets as it is free, but Since I was planning to collect the Air Pollution data for about a year, using Google Sheets was not a good idea. I was collecting about 900K rows of data every month.</p>
<h2 id="how-to-collect-data-using-api-in-python">How To Collect Data Using API In Python</h2>
<p>I would not bother writing this article again as I have already explained this in one of my articles, please check this out:</p>
<p>Fetching Data Through API in Python</p>
<p>How To Read And Write Data To Google Sheets Using Python</p>
<p>I have also written a detailed article for the same, Please check this out:</p>
<p>Reading And Writing Data in Google Sheets Using Python</p>
<p>I will suggest if your data is not going to be too huge and somewhere up to 500K to 800K rows of data, then you can definitely use Google Sheets. It is free and you can continue to collect the data as long as you want.</p>
<h2 id="how-to-create-a-database-on-aws-rds-and-connect-to-workbench">How To Create a Database on AWS RDS and Connect To Workbench</h2>
<p>In order to go to the next step of writing a Python Script to collect the data from the API, convert that to a Data frame and store append that data into a database, we will need to create a database first on AWS RDS. If you plan on using google sheets to collect your data, you can check out the article I have shared above in order to learn how to store data into google sheets using Python.<br>
In order to create a Database on AWS RDS, you will need to Sign Up for AWS, and if you are using AWS for the first time, you can use a lot of services on AWS for free for a year. That is really amazing, In fact, the data I collected for about a year, was during the free access time I had on my account.<br>
Check out this video made by me in order to see how you can create a database on RDS and connect that to Workbench on your system.</p>
<h2 id="how-to-collect-data-using-api-and-store-that-in-a-database-on-aws-rds">How To Collect Data Using API and Store That In a Database On AWS RDS</h2>
<p>If you decide to use create a MySQL Database on AWS RDS and want to store the data there, you should check out this code:</p>
<div class="joplin-editable"><pre class="joplin-source" data-joplin-language="python" data-joplin-source-open="```python&#10;" data-joplin-source-close="&#10;```">#Import all required packages
import requests
import pandas as pd
import io
import pygsheets
import datetime as dt
from datetime import timedelta
from sqlalchemy import create_engine
urlData =requests.get(&quot;https://api.data.gov.in/resource/use-your-key&quot;).content
#Converting the data to pandas dataframe.
rawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))
#Replace with proper name.
rawData = rawData.replace(to_replace =&quot;Andhra_Pradesh&quot;, value =&quot;Andhra Pradesh&quot;)
rawData = rawData.replace(to_replace =&quot;West_Bengal&quot;, value =&quot;West Bengal&quot;)
rawData = rawData.replace(to_replace =&quot;Uttar_Pradesh&quot;, value =&quot;Uttar Pradesh&quot;)
#Append blank row in the end of the dataframe.
rawData = rawData.append(pd.Series(), ignore_index=True)
now = dt.datetime.utcnow()+timedelta(hours=4, minutes=30)
now = now.strftime('%d-%m-%Y %H:00:00')
rawData['Measure_time'] = now
# changing columns using .columns() 
rawData.columns = ['Id', 'Country', 'State', 'City', 
                'Station', 'Last Update', 'Pollution ID',
'Pollutant Min', 'Pollutant Max', 'Pollutant Avg', 'Pollutant Unit', 'Measure_Time'] 
#Connection To Your Online Database begins
engine = create_engine(&quot;mysql://user:password@server&quot;
.format(user = &quot;your username&quot;,
pw = &quot;your password&quot;,
db=&quot;your database name&quot;))
#Insert whole DF into MySQL
df.to_sql('hourly_air_pollution', con=engine, if_exists='append', index=False)</pre><pre class="hljs"><code><span class="hljs-comment">#Import all required packages</span>
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> io
<span class="hljs-keyword">import</span> pygsheets
<span class="hljs-keyword">import</span> datetime <span class="hljs-keyword">as</span> dt
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta
<span class="hljs-keyword">from</span> sqlalchemy <span class="hljs-keyword">import</span> create_engine
urlData =requests.get(<span class="hljs-string">&quot;https://api.data.gov.in/resource/use-your-key&quot;</span>).content
<span class="hljs-comment">#Converting the data to pandas dataframe.</span>
rawData = pd.read_csv(io.StringIO(urlData.decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)))
<span class="hljs-comment">#Replace with proper name.</span>
rawData = rawData.replace(to_replace =<span class="hljs-string">&quot;Andhra_Pradesh&quot;</span>, value =<span class="hljs-string">&quot;Andhra Pradesh&quot;</span>)
rawData = rawData.replace(to_replace =<span class="hljs-string">&quot;West_Bengal&quot;</span>, value =<span class="hljs-string">&quot;West Bengal&quot;</span>)
rawData = rawData.replace(to_replace =<span class="hljs-string">&quot;Uttar_Pradesh&quot;</span>, value =<span class="hljs-string">&quot;Uttar Pradesh&quot;</span>)
<span class="hljs-comment">#Append blank row in the end of the dataframe.</span>
rawData = rawData.append(pd.Series(), ignore_index=<span class="hljs-literal">True</span>)
now = dt.datetime.utcnow()+timedelta(hours=<span class="hljs-number">4</span>, minutes=<span class="hljs-number">30</span>)
now = now.strftime(<span class="hljs-string">&#x27;%d-%m-%Y %H:00:00&#x27;</span>)
rawData[<span class="hljs-string">&#x27;Measure_time&#x27;</span>] = now
<span class="hljs-comment"># changing columns using .columns() </span>
rawData.columns = [<span class="hljs-string">&#x27;Id&#x27;</span>, <span class="hljs-string">&#x27;Country&#x27;</span>, <span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;City&#x27;</span>, 
                <span class="hljs-string">&#x27;Station&#x27;</span>, <span class="hljs-string">&#x27;Last Update&#x27;</span>, <span class="hljs-string">&#x27;Pollution ID&#x27;</span>,
<span class="hljs-string">&#x27;Pollutant Min&#x27;</span>, <span class="hljs-string">&#x27;Pollutant Max&#x27;</span>, <span class="hljs-string">&#x27;Pollutant Avg&#x27;</span>, <span class="hljs-string">&#x27;Pollutant Unit&#x27;</span>, <span class="hljs-string">&#x27;Measure_Time&#x27;</span>] 
<span class="hljs-comment">#Connection To Your Online Database begins</span>
engine = create_engine(<span class="hljs-string">&quot;mysql://user:password@server&quot;</span>
.<span class="hljs-built_in">format</span>(user = <span class="hljs-string">&quot;your username&quot;</span>,
pw = <span class="hljs-string">&quot;your password&quot;</span>,
db=<span class="hljs-string">&quot;your database name&quot;</span>))
<span class="hljs-comment">#Insert whole DF into MySQL</span>
df.to_sql(<span class="hljs-string">&#x27;hourly_air_pollution&#x27;</span>, con=engine, if_exists=<span class="hljs-string">&#x27;append&#x27;</span>, index=<span class="hljs-literal">False</span>)</code></pre></div>
<h1 id="2b-putting-the-python-script-on-a-virtual-machine-on-aws-ec2">2b. Putting The Python Script On a Virtual Machine on AWS EC2</h1>
<p>Once we have our python script ready for either getting the data and putting it on a Database on AWS RDS or on Google Sheets as per our requirement and wish, we need to put this script on a cloud platform. This script needs to run every hour and putting this on a Windows Task Scheduler or on a Cronjob on our local system will not be a good idea as our local system will not be on 24*7 (most probably), so we will need to set up a Virtual Machine on Cloud. You can use any cloud platform to achieve this task, but I will show you how to do this on AWS.<br>
I have created a video to explain this task, it will take merely 5–10 minutes if you are doing this for the first time, but if you know how to do it, it will take less than 2 minutes to set up a new Virtual Machine on EC2 and connect this via SSH to your local system.</p>
<h2 id="starting-a-virtual-machine-on-ec2-and-connecting-to-local-system-via-ssh-using-gitbash">Starting A Virtual Machine On EC2 and Connecting to Local System Via SSH Using Gitbash</h2>
<h2 id="creating-and-putting-your-python-script-on-cronjob-on-aws-machine">Creating And Putting Your Python Script On Cronjob On AWS Machine</h2>
<p>So till now, here is our progress:</p>
<ul>
<li>We have the Problem Statement ready.</li>
<li>We found an API to get the data for our problem statement. In order to collect the data, we either created a Database on AWS RDS or used a Google Sheet.</li>
<li>We wrote Python code to get the data from the API and append it in the Google Sheet or the Database.</li>
<li>We have put this script on AWS Machine on cronjob to make sure we are getting the data properly every hour.</li>
</ul>
<h1 id="step-3-data-cleaning">Step 3: Data Cleaning</h1>
<p>The good thing here for me is I can skip this step as I am collecting my data using an API. The data collected from the API are mostly in a cleaned format and you don’t have to spend time cleaning them. In my case, this is what my data looked like in the database:</p>
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*kQAoTGXPy8rVKoespIVq1Q.png" alt=""></p>
<p>Air Pollution Data Output</p>
<p>As you can see in the image, this is a pretty cleaned version of the data and I can use this for Analysis and Visualization without any further changes.</p>
<p>But in general Data Cleaning is a very huge topic and it can completely vary on the type of data you are dealing with. You might have to do the cleaning while collecting the data, this type of cleaning will be done in the code, like in my Python code where I was collecting the Air Pollution data, I had to do a bit of Cleaning. I wanted to use this Data in Tableau for Data Visualization, but the name of a few states was not coming in the correct format, because of this Tableau will not understand the names of the states and hence will not plot the locations correctly in India’s Map. Eg: Andhra Pradesh was coming as Andhra_Pradesh so on, so I wrote a small piece of code for this:</p>
<div class="joplin-editable"><pre class="joplin-source" data-joplin-language="python" data-joplin-source-open="```python&#10;" data-joplin-source-close="&#10;```">&lt;/a&gt;#Replace with proper name.
rawData = rawData.replace(to_replace =&quot;Andhra_Pradesh&quot;, value =&quot;Andhra Pradesh&quot;)
rawData = rawData.replace(to_replace =&quot;West_Bengal&quot;, value =&quot;West Bengal&quot;)
rawData = rawData.replace(to_replace =&quot;Uttar_Pradesh&quot;, value =&quot;Uttar Pradesh&quot;)</pre><pre class="hljs"><code>&lt;/a&gt;<span class="hljs-comment">#Replace with proper name.</span>
rawData = rawData.replace(to_replace =<span class="hljs-string">&quot;Andhra_Pradesh&quot;</span>, value =<span class="hljs-string">&quot;Andhra Pradesh&quot;</span>)
rawData = rawData.replace(to_replace =<span class="hljs-string">&quot;West_Bengal&quot;</span>, value =<span class="hljs-string">&quot;West Bengal&quot;</span>)
rawData = rawData.replace(to_replace =<span class="hljs-string">&quot;Uttar_Pradesh&quot;</span>, value =<span class="hljs-string">&quot;Uttar Pradesh&quot;</span>)</code></pre></div>
<p>Similarly, you might have to do the cleaning according to your data in the code itself.</p>
<p>If it is not possible to do the cleaning of the data in the source code, you might have to do this afterward at the time of querying the data from the database.</p>
<p>I will not deep dive into data cleaning at the moment because it is a very vast topic but I would provide you with links to some good articles which I was able to checkout related to Data Cleaning, I am sure it will be useful:<br>
1. <a data-from-md title='https://www.upgrad.com/blog/data-cleaning-techniques/' href='https://www.upgrad.com/blog/data-cleaning-techniques/'>Data Cleaning Techniques: Learn Simple &amp; Effective Ways To Clean Data</a><br>
2. <a data-from-md title='https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4' href='https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4'>The Ultimate Guide to Data Cleaning (Medium)</a><br>
3. <a data-from-md title='https://elitedatascience.com/data-cleaning' href='https://elitedatascience.com/data-cleaning'>Data Cleaning (Elite Data Science)</a><br>
4. <a data-from-md title='https://synthio.com/b2b-blog/5-best-practices-for-data-cleaning/' href='https://synthio.com/b2b-blog/5-best-practices-for-data-cleaning/'>5 Best Practices For Data Cleaning</a><br>
5. <a data-from-md title='https://www.digitalvidya.com/blog/data-cleaning-techniques/' href='https://www.digitalvidya.com/blog/data-cleaning-techniques/'>Data Cleaning Techniques</a></p>
<h2 id="learn-more-about-data-science-from-udacity-2"><a data-from-md title='https://imp.i115008.net/QOvQ13' href='https://imp.i115008.net/QOvQ13'>Learn more about Data Science from Udacity.</a></h2>
<p>** This is an affiliate link, meaning if you enroll in any course on Udacity using my URL, I will get a referral bonus.</p>
<h1 id="result-the-cleaned-data-for-analysis">Result: The Cleaned Data For Analysis</h1>
<p>After putting in all this effort, you get the cleaned version of your data for your analysis. Voila!!!<br>
I started collecting this data on 14th Oct 2019, but for a few days the data was not collected properly and there were few hours of data missing. I continued this collection of the data till September end 2020. In this period I collected a little over 10 million rows of data. I have published that data on the Kaggle Dataset for people to use and do the analysis.<br>
This dataset in CSV format was taking up around 1.2 GB, but I also created a .hyper extract (the Tableau data format) which was only about 122 MB files. I have uploaded the CSV and the Hyper file on the Kaggle Dataset website and you can download the same from <a data-from-md title='https://www.kaggle.com/prateekcoder/ind-hourly-air-pollution-data-oct2019-to-sept2020' href='https://www.kaggle.com/prateekcoder/ind-hourly-air-pollution-data-oct2019-to-sept2020'>here</a>:</p>
<p><a data-from-md title='https://www.kaggle.com/prateekcoder/ind-hourly-air-pollution-data-oct2019-to-sept2020' href='https://www.kaggle.com/prateekcoder/ind-hourly-air-pollution-data-oct2019-to-sept2020'>https://www.kaggle.com/prateekcoder/ind-hourly-air-pollution-data-oct2019-to-sept2020</a></p>
<p>This dataset can be used to analyze a lot of things like:</p>
<ol>
<li>How does the time period burning of crops by Haryana and Punjab Farmers affect air pollution?</li>
<li>Affect of festivals like Diwali and Dussehra on the air pollution level.</li>
<li>Is the pollution level less in winter months like January and February compared to November and December?</li>
<li>What was the effect of the Pandemic on the air pollution levels across India? ( This decreased drastically, we all know that but the detailed data will give you the exact picture.)</li>
</ol>
<h1 id="step-4-data-analysis">Step 4: Data Analysis</h1>
<p>Now we have the cleaned version of the data, I have provided the link to the CSV format and the Hyper format of the data. Now I will suggest you guys use this data to extract some insight from the data using the tools of your choice, it can be Excel, Python, Tableau, or any other tool. Being a Data Analyst the end goal is to solve the business problem, tools are just a medium to reach the destination.</p>
<p>For reference, let me show you a quick tableau Dashboard I built:</p>
<p><img src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*2g1nt55SzrgpQ0JTXfDFCA.png" alt=""></p>
<p>Tableau Dashboard</p>
<p><a data-from-md title='https://public.tableau.com/views/AirPollutionDataAnalysis/AirPollutionDataAnalysis?:language=en&amp;:embed=y&amp;:embed_code_version=3&amp;:loadOrderID=0&amp;:display_count=y&amp;publish=yes&amp;:origin=viz_share_link' href='https://public.tableau.com/views/AirPollutionDataAnalysis/AirPollutionDataAnalysis?:language=en&amp;:embed=y&amp;:embed_code_version=3&amp;:loadOrderID=0&amp;:display_count=y&amp;publish=yes&amp;:origin=viz_share_link'>https://public.tableau.com/views/AirPollutionDataAnalysis/AirPollutionDataAnalysis?:language=en&amp;:embed=y&amp;:embed_code_version=3&amp;:loadOrderID=0&amp;:display_count=y&amp;publish=yes&amp;:origin=viz_share_link</a></p>
<p>Now I built this Dashboard in 4–5 minutes and a few quick insights that I can derive from this visual are:</p>
<ol>
<li>The average PM 2.5 level in Delhi dropped from 388 in December 2020 to 74.6 in April 2020.</li>
<li>After major Unlocks in Delhi in August, the PM 2.5 level can be seen increasing significantly.</li>
<li>There was no major effect of lockdown on pollutant metric Ozone as the highest average value can be seen in May 2020. So we will need to dig deeper to find the variables having a good correlation with Ozone pollutant id.</li>
</ol>
<p>There is other information as well that can be drawn from the visual and a much better and more informative Dashboard can be built using this data. I will leave that for you guys to brainstorm and come up with something better.</p>
<p>I hope this article helps, Happy Learning!!!</p>
</div></div>
					</body>
				</html>
			